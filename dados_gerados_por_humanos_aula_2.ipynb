{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d6ca84",
   "metadata": {},
   "source": [
    "#### **Alterar para o nome do módulo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580d69f",
   "metadata": {},
   "source": [
    "#### **Observações**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dfb055",
   "metadata": {},
   "source": [
    "##### **Introdução e Técnicas de Processamento de Texto**\n",
    "\n",
    "O campo de processamento de linguagem natural (NLP) passou por uma mudança dramática nos últimos anos, tanto em termos de metodologia quanto em termos de aplicativos suportados. Os avanços metodológicos têm variado desde novas formas de representar documentos a novas técnicas de síntese de linguagem. Com eles, surgiram novos aplicativos que vão desde sistemas de conversação abertos até técnicas que usam linguagem natural para a interpretabilidade do modelo. Por fim, esses avanços permitiram que a NLP ganhasse espaço em áreas relacionadas, como visão computacional e sistemas de recomendação. Este último será objeto de estudo nosso futuramente.\n",
    "\n",
    "Num sentido amplo, Processamento de Linguagem Natural (NLP) trata de qualquer tipo de manipulação computacional de linguagem natural, desde uma simples contagem de frequências de palavras para comparar diferentes estilos de escrita, até o “entendimento” completo de interações humanas (pelo menos no sentido de oferecer uma resposta útil a eles).\n",
    "\n",
    "As tecnologias baseadas em NLP estão se tornando cada vez mais pervasivas e, diante das interfaces homem-máquina mais naturais e meios mais sofisticados de armazenamento de informações, o processamento de linguagem tem alcançado um papel central numa sociedade da informação multilíngue.\n",
    "\n",
    "Por conta disso, NLP está rapidamente se tornando uma habilidade necessária exigida por engenheiros, gerentes de produto, cientistas, estudantes e entusiastas que desejam construir aplicativos com base em dados de linguagem natural. Por um lado, novas ferramentas e bibliotecas, para NLP e aprendizado de máquina tornaram a modelagem de linguagem natural mais acessível do que nunca. Mas, por outro lado, os recursos para aprender NLP devem visar esse público diversificado e sempre crescente.\n",
    "\n",
    "Como dito, NLP permite interação com sistemas computacionais em linguagem humana. Entretanto, computadores entendem apenas dados binários, por exemplo, 0 e 1. \n",
    "\n",
    "Para exemplificar o quão importante NLP se tornou em nossa vida, aqui vão algumas aplicações:\n",
    "\n",
    "1. Plataformas de e-mail usam NLP para classificar mensagens (spam ou legítimas), priorização na caixa de entrada e auto-complete;\n",
    "2. Assistentes baseados em voz, tais como Amazon Alexa, Apple Siri, Google Assistant ou Microsoft Cortana são baseados em técnicas de NLP para interagir com os usuários, entende-los e responde-los corretamente;\n",
    "3. Plataformas de busca (Search engine), como Google ou Bing, usam NLP para entendimento de query (informação que o usuário digitou), recuperação da informação e ranqueamento, para citar alguns;\n",
    "4. Tradução de máquina, como o Google Translate, é construído em cima de técnicas de NLP\n",
    "5. Além disso, NLP pode ser usado em uma variedade de campos, como jurídico, saúde, varejo, atendimento, marketing e outros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d2498",
   "metadata": {},
   "source": [
    "##### **Pipeline de NLP**\n",
    "\n",
    "Quando falamos de NLP, geralmente usamos técnicas de Machine Learning. Elas são aplicadas a dados textuais da mesma maneira que são usadas em outros tipos de dados, como imagens ou dados estruturados. \n",
    "\n",
    "Toda abordagem de Machine Learning para NLP, seja ela supervisionada ou não supervisionada, pode ser descrita em três passos comuns:\n",
    "\n",
    "1. Extrair features de um texto.\n",
    "2. Usar uma representação dessas features para aprender um modelo.\n",
    "3. Avaliar e melhorar o modelo.\n",
    "\n",
    " primeiro passo é o de “aquisição de dados”, que consiste em obter os dados textuais que serão usados para treinar nosso modelo de Machine Learning. Geralmente, os dados estão disponíveis em bancos de dados nas próprias empresas. Às vezes, você encontra algumas bases públicas, em outros casos, você precisa ir capturando as informações aos poucos. Essa parte é importante, apesar de não ser o foco da disciplina, pois quanto mais dados, melhor será o nosso modelo. \n",
    "\n",
    "No segundo passo, iremos fazer a limpeza desse texto, removendo qualquer coisa que não seja texto, tais como metadados, links, entre outros. O terceiro passo é o pré-processamento. Às vezes, ele é executado junto com a limpeza dos dados, consistindo numa série de técnicas que serão objeto de estudo dessa aula. \n",
    "\n",
    "O próximo passo, também conhecido como “feature extraction”, consiste em extrair do texto quais características melhor o descrevem e imputá-las em algoritmos de machine learning. A parte de modelagem consiste em escolher uma técnica de machine learning e usar os dados tratados para resolver um problema específico. Logo em seguida, devemos avaliar esse modelo, a partir de alguns critérios que apresentaremos no decorrer da disciplina. \n",
    "\n",
    "Por fim, faremos o deploy do nosso modelo, disponibilizando o modelo treinado em ambiente produtivo e, frequentemente, monitorando desvios, erros e realizando correções, quando necessário, treinando o modelo novamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75cc7c2",
   "metadata": {},
   "source": [
    "##### **Técnicas de Pré-Processamento**\n",
    "\n",
    "Como dito anteriormente, o primeiro passo a ser feito em NLP é extrair features de um texto. Mais precisamente, precisamos adquirir os dados, limpá-los, fazer o pré-processamento e extrair as características, transformando os dados para que possam ser imputados em modelos de machine learning. Vamos tratar desses passos nessa sessão. Importante dizer que, a título de didática, consideramos que já temos os dados disponíveis. \n",
    "\n",
    "Como sabemos, os computadores lidam com números. Qualquer outro tipo de dado (como áudio, texto, imagens, etc.) precisa passar por um processo de transformação para números. \n",
    "\n",
    "Para realizar essa transformação, antes precisamos fazer o pré-processamento de dados, cujo objetivo é preparar os textos para criar um espaço de características adequado. \n",
    "\n",
    "O primeiro passo é a tokenização dos dados. A tokenização nada mais é que uma sequência de “n” elementos de uma sequência maior, denominadas n-gramas. Seu objetivo é transformar os textos em números. Os tipos de n-grama são definidos pela quantidade de elementos que os compõem. Unigramas (N = 1), Bigramas (N = 2) e Trigramas (N = 3).\n",
    "\n",
    "A ideia é dividir o texto, geralmente usando “espaço” como separador, em tokens, para realizar outras atividades.\n",
    "Considere as seguintes frases como exemplos: \n",
    "\n",
    "* Eu gosto de assistir jogos de futebol.\n",
    "* Já eu, prefiro assistir jogos de basquete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae1c17",
   "metadata": {},
   "source": [
    "##### **POS-Tagger**\n",
    "\n",
    "Na escola primária, aprendemos a diferença entre substantivo, verbos, advérbios e adjetivos. Tais classes gramaticais são categorias úteis para muitas tarefas de processamento de linguagem natural. POS-Tagging é uma ferramenta usada no processamento de linguagem natural (NLP), que permite que os algoritmos entendam a estrutura gramatical de uma frase e desambiguam palavras que têm vários significados.\n",
    "\n",
    "Aqui, teremos os seguintes objetivos:\n",
    "\n",
    "* Quais são as categorias léxicas (classes gramaticais) e como elas são usadas em NLP.\n",
    "* Uma boa estrutura de dados em Python, para armazenar palavras e suas categorias.\n",
    "* Como podemos marcar (taguear) automaticamente cada palavra de um texto com sua classe.\n",
    "\n",
    "Observe a imagem a seguir:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59115f",
   "metadata": {},
   "source": [
    "#### **Conteúdo - Bases e Notebook da aula**\n",
    "\n",
    "Github:  \n",
    "https://github.com/FIAP/Pos_Tech_DTAT/tree/main/Fase%205/Dados%20gerados%20por%20humanos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770d68e",
   "metadata": {},
   "source": [
    "#### **Importação de bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7255822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ricar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ricar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ricar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\ricar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\ricar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ricar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package floresta to\n",
      "[nltk_data]     C:\\Users\\ricar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package floresta is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importar biblioteca completa\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "nltk.corpus.stopwords.words('portuguese')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('rslp')\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('floresta')\n",
    "\n",
    "# Importar algo especifico de uma biblioteca\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.rslp import RSLPStemmer\n",
    "from nltk.corpus import floresta\n",
    "from pickle import dump\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672ffbc",
   "metadata": {},
   "source": [
    "#### **Funções (def)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "493b2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_tag(t):\n",
    "  if \"+\" in t:\n",
    "    return t.split(\"+\")[1]\n",
    "  return t "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a6a49a",
   "metadata": {},
   "source": [
    "#### **Aula 2 - Introdução e Técnicas de Processamento de Texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b4d29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "66a462a2-42bd-4e8c-9b39-4ecec78113d7",
       "rows": [
        [
         "0",
         "Eu gosto de assistir jogos de futebol"
        ],
        [
         "1",
         "Já eu, prefiro assistir jogos de basquete"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eu gosto de assistir jogos de futebol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Já eu, prefiro assistir jogos de basquete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text\n",
       "0      Eu gosto de assistir jogos de futebol\n",
       "1  Já eu, prefiro assistir jogos de basquete"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'text': [\n",
    "      'Eu gosto de assistir jogos de futebol',\n",
    "      'Já eu, prefiro assistir jogos de basquete'\n",
    "    ]\n",
    "    })\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ff3f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0  1\n",
      "assistir  1  1\n",
      "basquete  0  1\n",
      "de        2  1\n",
      "eu        1  1\n",
      "futebol   1  0\n",
      "gosto     1  0\n",
      "jogos     1  1\n",
      "já        0  1\n",
      "prefiro   0  1\n"
     ]
    }
   ],
   "source": [
    "# Unigrama\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.toarray(), columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ee001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0  1\n",
      "assistir jogos    1  1\n",
      "de assistir       1  0\n",
      "de basquete       0  1\n",
      "de futebol        1  0\n",
      "eu gosto          1  0\n",
      "eu prefiro        0  1\n",
      "gosto de          1  0\n",
      "jogos de          1  1\n",
      "já eu             0  1\n",
      "prefiro assistir  0  1\n"
     ]
    }
   ],
   "source": [
    "# Bigramas\n",
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.toarray(), columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07222548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0  1\n",
      "assistir jogos de       1  1\n",
      "de assistir jogos       1  0\n",
      "eu gosto de             1  0\n",
      "eu prefiro assistir     0  1\n",
      "gosto de assistir       1  0\n",
      "jogos de basquete       0  1\n",
      "jogos de futebol        1  0\n",
      "já eu prefiro           0  1\n",
      "prefiro assistir jogos  0  1\n"
     ]
    }
   ],
   "source": [
    "# Trigramas\n",
    "vect = CountVectorizer(ngram_range=(3,3))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.toarray(), columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bdbeb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'futebol',\n",
       " 'brasileiro',\n",
       " 'é',\n",
       " 'o',\n",
       " 'melhor',\n",
       " 'do',\n",
       " 'mundo',\n",
       " '.',\n",
       " 'Você',\n",
       " 'concorda',\n",
       " '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemplo = 'O futebol brasileiro é o melhor do mundo. Você concorda?'\n",
    "words = word_tokenize(exemplo)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceec613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queen, Aerosmith & Beatles -> ['Queen', 'Aerosmith', 'Beatles']\n",
      "Phone Num :  2004-959-559 \n",
      "Phone Num :  2004959559\n"
     ]
    }
   ],
   "source": [
    "# Trabalhando com Regex\n",
    "rex = re.compile('\\w+') \n",
    "bandas = 'Queen, Aerosmith & Beatles'\n",
    "print (bandas, '->', rex.findall(bandas))\n",
    "phone = \"2004-959-559 # This is Phone Number\"\n",
    "num = re.sub('#.*$', \"\", phone) \n",
    "print (\"Phone Num : \", num)\n",
    "num = re.sub(r'\\D', \"\", phone)\n",
    "print (\"Phone Num : \", num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "719e3140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'carro', 'que', 'estava', 'quebrado', 'voltou', 'a', 'funcionar']\n",
      "['Meu', 'carro', 'quebrou', 'e', 'não', 'está', 'funcionando']\n"
     ]
    }
   ],
   "source": [
    "ex1 = 'O carro que estava quebrado voltou a funcionar'\n",
    "ex2 = 'Meu carro quebrou e não está funcionando'\n",
    "\n",
    "print(word_tokenize(ex1))\n",
    "print(word_tokenize(ex2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52d0dce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  1\n",
      "carro        1  1\n",
      "estava       1  0\n",
      "está         0  1\n",
      "funcionando  0  1\n",
      "funcionar    1  0\n",
      "meu          0  1\n",
      "não          0  1\n",
      "que          1  0\n",
      "quebrado     1  0\n",
      "quebrou      0  1\n",
      "voltou       1  0\n"
     ]
    }
   ],
   "source": [
    "# Criar dicionario\n",
    "df = pd.DataFrame({'text':[ex1,ex2]})\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.toarray(), columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dada6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  1\n",
      "carro        1  1\n",
      "funcionando  0  1\n",
      "funcionar    1  0\n",
      "quebrado     1  0\n",
      "quebrou      0  1\n",
      "voltou       1  0\n"
     ]
    }
   ],
   "source": [
    "# Redução de dimensionalidade\n",
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.toarray(), columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5542fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "go\n",
      "go\n",
      "go\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "   \"go\",\"going\",\n",
    "   \"goes\",\"gone\",\"went\"\n",
    "]\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for word in examples:\n",
    "  print(wnl.lemmatize(word, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "339c2165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect\n",
      "connect\n",
      "connect\n",
      "connect\n",
      "connect\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"connection\",\"connections\",\n",
    "    \"connective\",\"connecting\",\"connected\"\n",
    "]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in examples:\n",
    "  print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db404114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conecta\n",
      "conectado\n",
      "conectamo\n",
      "desconectado\n",
      "conectividad\n"
     ]
    }
   ],
   "source": [
    "examples = [\"conecta\",\"conectado\",\"conectamos\",\"desconectados\",\"conectividade\"]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in examples:\n",
    "  print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8115f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conect\n",
      "conect\n",
      "conect\n",
      "desconect\n",
      "conect\n"
     ]
    }
   ],
   "source": [
    "examples = [\"conecta\",\"conectado\",\"conectamos\",\"desconectados\",\"conectividade\"]\n",
    "\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "for word in examples:\n",
    "  print(rslp.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10a5a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem1 = \" \".join([rslp.stem(x) for x in word_tokenize(ex1)])\n",
    "stem2 = \" \".join([rslp.stem(x) for x in word_tokenize(ex2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23bedb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  1\n",
      "carr     1  1\n",
      "est      1  1\n",
      "funcion  1  1\n",
      "quebr    1  1\n",
      "volt     1  0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'text':[stem1,stem2]})\n",
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.toarray(), columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f523559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68a76115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time day year car moment world house family child country boy\n",
      "state job place way war girl work word\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "text.similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f28799f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made said done put had seen found given left heard was been brought\n",
      "set got that took in told felt\n"
     ]
    }
   ],
   "source": [
    "text.similar('bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef7d072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in on to of and for with from at by that into as up out down through\n",
      "is all about\n"
     ]
    }
   ],
   "source": [
    "text.similar('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3221ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a his this their its her an that our any all one these my in your no\n",
      "some other and\n"
     ]
    }
   ],
   "source": [
    "text.similar('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ab66c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Um', '>N+art'), ('revivalismo', 'H+n'), ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floresta.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc2786f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('um', 'art'),\n",
       " ('revivalismo', 'n'),\n",
       " ('refrescante', 'adj'),\n",
       " ('o', 'art'),\n",
       " ('7_e_meio', 'prop'),\n",
       " ('é', 'v-fin'),\n",
       " ('um', 'art'),\n",
       " ('ex-libris', 'n'),\n",
       " ('de', 'prp'),\n",
       " ('a', 'art')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twords = nltk.corpus.floresta.tagged_words()\n",
    "twords = [(w.lower(),simplify_tag(t)) for (w,t) in twords]\n",
    "twords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c62749b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [tag for (word, tag) in twords]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8953d75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Esse', 'n'),\n",
       " ('é', 'n'),\n",
       " ('um', 'n'),\n",
       " ('exemplo', 'n'),\n",
       " ('utilizando', 'n'),\n",
       " ('o', 'n'),\n",
       " ('marcador', 'n'),\n",
       " ('padrão', 'n')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = 'Esse é um exemplo utilizando o marcador padrão'\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "default_tagger = nltk.DefaultTagger('n')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "162c81c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('o', 'art'),\n",
       "  ('7_e_meio', 'prop'),\n",
       "  ('é', 'v-fin'),\n",
       "  ('um', 'art'),\n",
       "  ('ex-libris', 'n'),\n",
       "  ('de', 'prp'),\n",
       "  ('a', 'art'),\n",
       "  ('noite', 'n'),\n",
       "  ('algarvia', 'adj'),\n",
       "  ('.', '.')],\n",
       " [('é', 'v-fin'),\n",
       "  ('uma', 'num'),\n",
       "  ('de', 'prp'),\n",
       "  ('as', 'art'),\n",
       "  ('mais', 'adv'),\n",
       "  ('antigas', 'adj'),\n",
       "  ('discotecas', 'n'),\n",
       "  ('de', 'prp'),\n",
       "  ('o', 'art'),\n",
       "  ('algarve', 'prop'),\n",
       "  (',', ','),\n",
       "  ('situada', 'v-pcp'),\n",
       "  ('em', 'prp'),\n",
       "  ('albufeira', 'prop'),\n",
       "  (',', ','),\n",
       "  ('que', 'pron-indp'),\n",
       "  ('continua', 'v-fin'),\n",
       "  ('a', 'prp'),\n",
       "  ('manter', 'v-inf'),\n",
       "  ('os', 'art'),\n",
       "  ('traços', 'n'),\n",
       "  ('decorativos', 'adj'),\n",
       "  ('e', 'conj-c'),\n",
       "  ('as', 'art'),\n",
       "  ('clientelas', 'n'),\n",
       "  ('de', 'prp'),\n",
       "  ('sempre', 'adv'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsents = floresta.tagged_sents()\n",
    "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in tsents if sent]\n",
    "train = tsents[1000:]\n",
    "test = tsents[:1000]\n",
    "tsents[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d9eee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17800040072129833\n"
     ]
    }
   ],
   "source": [
    "tagger0 = nltk.DefaultTagger('n')\n",
    "print(tagger0.accuracy(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de4cc68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8522139851733119\n"
     ]
    }
   ],
   "source": [
    "# Unigram Tagger\n",
    "tagger1 = nltk.UnigramTagger(train)\n",
    "print(tagger1.accuracy(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f16a55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14626327389300742\n"
     ]
    }
   ],
   "source": [
    "tagger2 = nltk.BigramTagger(train)\n",
    "print(tagger2.accuracy(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fbfcf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagger1:  0.8740532959326788\n",
      "tagger2:  0.8900420757363254\n"
     ]
    }
   ],
   "source": [
    "tagger1 = nltk.UnigramTagger(train, backoff=tagger0)\n",
    "print('tagger1: ',tagger1.accuracy(test))\n",
    "\n",
    "tagger2 = nltk.BigramTagger(train, backoff=tagger1)\n",
    "print('tagger2: ',tagger2.accuracy(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6433c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('tagger.pkl', 'wb')\n",
    "dump(tagger2, output, -1)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77de60d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = open('tagger.pkl', 'rb')\n",
    "tagger = load(input)\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5fa524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1:  [('Isso', 'n'), ('é', 'v-fin'), ('para', 'prp'), ('você.', 'n')]\n",
      "text2:  [('para', 'prp'), ('com', 'prp'), ('isso', 'pron-indp')]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Isso é para você.\"\n",
    "text2 = \"para com isso\"\n",
    "tokens1 = text1.split()\n",
    "tokens2 = text2.split()\n",
    "print('text1: ',tagger.tag(tokens1))\n",
    "print('text2: ',tagger.tag(tokens2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fase-5-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

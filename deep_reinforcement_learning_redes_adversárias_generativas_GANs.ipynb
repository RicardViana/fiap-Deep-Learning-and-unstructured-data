{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d6ca84",
   "metadata": {},
   "source": [
    "#### **Deep & Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580d69f",
   "metadata": {},
   "source": [
    "#### **Observa√ß√µes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a53df29",
   "metadata": {},
   "source": [
    "##### Redes Advers√°rias Generativas (GANs)\n",
    "\n",
    "Vamos aprender a aplicar as Redes Advers√°rias Generativas (GANs)? Vamos utilizar a cl√°ssica base de dados MNIST. Esse dataset √© disponibilizado pelo Keras e consistem em imagens de treinamento e imagens de teste para classificar d√≠gitos escritos a m√£o que variam entre 0 a 9.\n",
    "\n",
    "O que s√£o as redes advers√°rias gerativas? ü§î\n",
    "\n",
    "As redes Advers√°rias Generativas possuem um grande potencial pois s√£o capazes de gerar novos dados a partir de um conjunto de dados treinados. Podemos treinar essa rede para criar, por exemplo, novas imagens, m√∫sicas, falas, prosas, tratar resolu√ß√µes de imagens e v√≠deos e muito mais. Uma das suas utilidades tamb√©m pode ser criar novas imagens a partir de um conjunto de dados real para criar mais amostras de dados ao treinar uma rede neural convolucional.\n",
    "\n",
    "Voc√™ j√° viu aquelas imagens fakes? Pois √©, as redes GANs tem tanto o poder de criar dados fakes quanto validar a veracidade de dados. A rede GANs tamb√©m √© muito utilizada para recriar partes de imagens, por exemplo, se temos uma imagem de um cachorro pela metade, a rede tem o incr√≠vel poder de recriar. Falando ainda do uso da rede em imagens, a Pixar que √© uma grande empresa que trabalha com anima√ß√µes utiliza muito as redes GANs para aumento de resolu√ß√£o de imagens. Para quem n√£o conhece as redes Advers√°rias Generativas at√© parece m√°gica, n√£o √© mesmo?\n",
    "\n",
    "Voc√™ aprendeu na aula de redes n√£o supervisionadas que algoritmos discriminativos tentam criar classes ou grupos a partir de dados de entrada, ou seja, mapeiam recursos para criar r√≥tulos utilizando correla√ß√£o. Os algoritmos generativos fazem justamente o oposto, eles tentam prever os recursos (dados) com um determinado r√≥tulo. Por exemplo, dado que um e-mail √© classificado com spam, qual √© a probabilidade de palavras que formam esse e-mail spam? Os algoritmos discriminativos se preocupam com a correla√ß√£o entre x e y, modelos generativos se preocupam em ‚Äúcomo voc√™ ir√° obter x‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831837de",
   "metadata": {},
   "source": [
    "##### **Como criar ambientes para evitar conflito:**\n",
    "https://github.com/RicardViana/fiap-data-viz-and-production-models/blob/main/Roteiro%20para%20cria%C3%A7%C3%A3o%20de%20ambiente.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59115f",
   "metadata": {},
   "source": [
    "#### **Conte√∫do - Bases e Notebook da aula**\n",
    "\n",
    "Github:  \n",
    "https://github.com/FIAP/Pos_Tech_DTAT/tree/DeepLearning\n",
    "\n",
    "ou\n",
    "\n",
    "https://github.com/FIAP/Pos_Tech_DTAT/tree/main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770d68e",
   "metadata": {},
   "source": [
    "#### **Importa√ß√£o de bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar biblioteca completa\n",
    "import os\n",
    "import glob     \n",
    "import time\n",
    "import imageio  \n",
    "import PIL      \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importar algo especifico de uma biblioteca\n",
    "from pathlib import Path\n",
    "from IPython import display # Para limpar a sa√≠da do notebook e atualizar a imagem \"ao vivo\"\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Reshape, Conv2DTranspose, Conv2D, LeakyReLU, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tqdm import tqdm \n",
    "\n",
    "print(f\"TensorFlow vers√£o: {tf.__version__}\")\n",
    "print(\"Ambiente configurado para criar, treinar e gerar GIFs da GAN!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672ffbc",
   "metadata": {},
   "source": [
    "#### **Fun√ß√µes (def)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e5ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "Basicamente as GANs s√£o compostas por duas redes, a geradora e a discriminante. S√£o chamadas de advers√°rias porque essas duas redes tentam ‚Äújogar‚Äù uma com a outra o tempo todo.\n",
    "A rede geradora tenta enganar a rede discriminante gerando dados fakes semelhantes aos dados reais.\n",
    "\n",
    "O objetivo da rede discriminate aqui √© reconhecer que os d√≠gitos gerados a m√£o s√£o o mais pr√≥ximos poss√≠veis dos verdadeiros n√∫meros. \n",
    "A rede geradora tenta criar novas imagens fakes com a esperan√ßa de torn√°-las aut√™nticas tamb√©m (mesmo sendo falsas). \n",
    "O funcionamento da rede consiste nas seguintes etapas: O gerador considera n√∫meros aleat√≥rios e retornam uma imagem (ou seja, cria uma imagem fake); \n",
    "Essa imagem fake gerada pelo gerador √© inserida no discriminador ao lado do fluxo de imagens verdadeiras geradas; o discriminador obt√©m imagens reais e falsas retornando \n",
    "a probabilidade realizando previs√µes de imagens falsas geradas pela rede generativa.\n",
    "\n",
    "'''\n",
    "\n",
    "# criando o nosso modelo gerador\n",
    "\n",
    "def make_generator_model():\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "\n",
    "  model.add(layers.Reshape((7,7,256)))\n",
    "  assert model.output_shape == (None, 7, 7, 256) # None √© o batch size\n",
    "\n",
    "  model.add(layers.Conv2DTranspose(128,(5, 5), strides=(1,1), padding='same', use_bias=False))\n",
    "  assert model.output_shape == (None, 7, 7, 128)\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "\n",
    "  model.add(layers.Conv2DTranspose(64,(5, 5), strides=(2,2), padding='same', use_bias=False))\n",
    "  assert model.output_shape == (None, 14, 14, 64)\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "\n",
    "  model.add(layers.Conv2DTranspose(1,(5, 5), strides=(2,2), padding='same', use_bias=False, activation='tanh'))\n",
    "  assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5988205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(layers.Conv2D(64, (5, 5), strides=(2,2), padding='same', input_shape=[28,28,1]))\n",
    "  model.add(layers.LeakyReLU())\n",
    "  model.add(layers.Dropout(0.3))\n",
    "\n",
    "  model.add(layers.Conv2D(128, (5, 5), strides=(2,2), padding='same'))\n",
    "  model.add(layers.LeakyReLU())\n",
    "  model.add(layers.Dropout(0.3))\n",
    "\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(1))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec6b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fun√ß√£o loss para o discriminador\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "  real_loss = cross_entropy(tf.ones_like(real_output), real_output) # dado real # 1 verdadeiro\n",
    "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output) # dado fake # 0 falso\n",
    "  total_loss = real_loss + fake_loss # faz a compara√ß√£o\n",
    "  \n",
    "  return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliar fake gerado\n",
    "def generator_loss(fake_output):\n",
    "  return cross_entropy(tf.ones_like(fake_output ), fake_output) # dado fake apenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando uma fun√ß√£o para mostrar a evolu√ß√£o dos passos de treinamento\n",
    "\n",
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fun√ß√£o para definir as imagens\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac42bf",
   "metadata": {},
   "source": [
    "#### **Aula 6 - Gans**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subindo a base de dados\n",
    "(train_images, train_labels), (_,_) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar as imagens\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32') # definindo o tamaho da imagm (28x28) com 1 canal de cinza\n",
    "train_images = (train_images - 127.5) / 127.5 # normalizando a imagem para -1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set do tamanho do conjuto de dados e a qtd de pixels\n",
    "BUFFER_SIZE = 60000 # tamanho do conjunto de dados\n",
    "BATCH_SIZE = 256 # quantidade de pixels que varia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd7a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando a base de treinamento\n",
    "# from_tensor_slices: imagens dentro de um tensor\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE) #conjunto de dados + batch para normalizar os pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c743e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Agora utilizando a fun√ß√£o geradora de imagens, vamos ver o que ela pode criar de forma aleat√≥ria.\n",
    "Criando a primeira imagem nunca treinada antes:\n",
    "\n",
    "'''\n",
    "\n",
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generator_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generator_image[0, :, :, 0], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generator_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando a fun√ß√£o de custo (loss function)\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False) # crio uma fun√ß√£o loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09cd6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o otimizador da fun√ß√£o de custo Adam\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para salvar o modelo\n",
    "checkpoint_dir = './training_checkpoints' #diret√≥rio para salvar pontos espec√≠ficos de rede\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                               discriminator_optimizer=discriminator_optimizer,\n",
    "                                generator=generator,\n",
    "                                discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d1fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir as √©pocas de processamento:\n",
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cde9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiozar o treino:\n",
    "train(train_dataset, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambiente_fase_5_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

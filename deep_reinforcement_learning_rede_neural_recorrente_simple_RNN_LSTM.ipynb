{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d6ca84",
   "metadata": {},
   "source": [
    "#### **Deep & Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580d69f",
   "metadata": {},
   "source": [
    "#### **Observa√ß√µes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e88704",
   "metadata": {},
   "source": [
    "##### Classificando not√≠cias com Redes Neurais Recorrentes üì∞\n",
    "Imagine que temos o desafio de organizar diversos tipos de not√≠cias por assuntos, como voc√™ faria? Basicamente podemos classificar uma not√≠cia dado algum contexto, por exemplo:\n",
    "\n",
    "\"Pela quarta rodada do Campeonato Italiano, a Roma n√£o tomou conhecimento do Empoli neste domingo e venceu pela primeira vez na competi√ß√£o.\"\n",
    "\n",
    "Com base nesse contexto, poderiamos classificar essa not√≠cia como do tipo \"Esportes\", certo? E por que sabemos disso? Bem, podemos observar algumas palavrinhas chaves tal como \"campeonato\" e \"competi√ß√£o\".\n",
    "\n",
    "Nessa aula, temos o desafio de ensinar uma rede neural recorrente realizar esse tipo de trabalho! Classificar not√≠cias com base em textos.\n",
    "\n",
    "Como bem j√° sabemos, as redes neurais recorrentes aprende com ela mesma (assim como n√≥s humanos aprendemos com nossos erros). Basicamente, esse tipo de arquitetura aprende n√£o s√≥ com os dados de entrada mas tamb√©m com as pr√≥prias sa√≠das da rede (muito parecido com um looping de aprendizado, por isso chamamos de redes recorrentes). Como nesse cen√°rio precisamos de uma sequ√™ncia de palavras para fazer sentido ao contexto, as RNNs podem ser uma boa alternativa! Vamos codar? üòÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98f831",
   "metadata": {},
   "source": [
    "##### **Limpeza dos dados** üßπ\n",
    "\n",
    "Antes de iniciar as etapas de pr√©-processamento que citei anteriormente, vamos precisar limpar os dados. Vamos ciar uma fun√ß√£o que realize a etapa de limpeza do texto utilizando alguns REGEX, remo√ß√£o de stop words e lematizar algumas palavras.\n",
    "\n",
    "Para contextuallizar o que fazem essas t√©cnicas, vamos falar um pouquinho mais sobre elas.\n",
    "\n",
    "**Regex**  \n",
    "Pode ser definida como uma **forma flex√≠vel de identificar determinada cadeia de caractere** para nosso interesse. Uma cadeia pode ser um caractere espec√≠fico, uma palavra ou um padr√£o.\n",
    "No Python, o m√≥dulo **re** prov√™ um analisador sint√°tico que permite o uso de tais express√µes. Os padr√µes definidos atrav√©s de caracteres que tem significado especial para o analisador.\n",
    "\n",
    "**Stop-words**  \n",
    "Essa t√©cnica consiste na **remo√ß√£o de ru√≠dos** do texto que s√£o menos evidentes que pontua√ß√µes, como os conectivos ‚Äúque‚Äù, ‚Äúo‚Äù, ‚Äúa‚Äù, ‚Äúde‚Äù, entre outros. Normalmente √© um conjunto composto por artigos, adv√©rbios, preposi√ß√µes e alguns verbos.\n",
    "\n",
    "**Lematiza√ß√£o**  \n",
    "Basicamente √© um processo que determina uma √∫nica ‚Äúraiz‚Äù para a palavra, independente de suas diferen√ßas superficiais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a4fba",
   "metadata": {},
   "source": [
    "##### **Entrando no preparo dos dados!** üìè\n",
    "\n",
    "**Tokeniza√ß√£o e vetoriza√ß√£o**  \n",
    "\n",
    "**Tokeniza√ß√£o**  \n",
    "A tokeniza√ß√£o √© uma etapa inicial no processo de NLP para dividir frases de texto em palavras ou tokens menores. Por exemplo: \"O time venceu a partida\" ficaria \"o\", \"time\", \"venceu\", \"a\", \"partida\".\n",
    "\n",
    "E porque precisamos tokenizar?\n",
    "\n",
    "√â necess√°rio para identficar cada uma das palavras contidas na base.\n",
    "\n",
    "**Vetoriza√ß√£o**  \n",
    "A m√°quina n√£o entende texto ou palavras, portanto, dados de texto ou tokens devem ser **convertidos em √≠ndices de palavras ou vetores de palavras** para processar texto e construir modelos. Por exemplo,  \"o: 1\", \"time: 2\", \"venceu: 3\", \"a: 4\", \"partida: 5\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831837de",
   "metadata": {},
   "source": [
    "##### **Como criar ambientes para evitar conflito:**\n",
    "https://github.com/RicardViana/fiap-data-viz-and-production-models/blob/main/Roteiro%20para%20cria%C3%A7%C3%A3o%20de%20ambiente.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59115f",
   "metadata": {},
   "source": [
    "#### **Conte√∫do - Bases e Notebook da aula**\n",
    "\n",
    "Github:  \n",
    "https://github.com/FIAP/Pos_Tech_DTAT/tree/DeepLearning\n",
    "\n",
    "ou\n",
    "\n",
    "https://github.com/FIAP/Pos_Tech_DTAT/tree/main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770d68e",
   "metadata": {},
   "source": [
    "#### **Importa√ß√£o de bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar biblioteca completa\n",
    "import config\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Importar algo especifico de uma biblioteca\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import STOPWORDS\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from keras.layers import Conv1D, SimpleRNN, Bidirectional, MaxPooling1D, GlobalMaxPool1D, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import L1L2\n",
    "from keras.src.legacy.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672ffbc",
   "metadata": {},
   "source": [
    "#### **Fun√ß√µes (def)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex para limpar o texto\n",
    "def limpeza_texto(text):\n",
    "    \n",
    "    whitespace = re.compile(r\"\\s+\")                            # encontrando espa√ßos em branco\n",
    "    user = re.compile(r\"(?i)@[a-z0-9_]+\")                      # encontrar men√ß√µes de usu√°rios, exemplo @usuario\n",
    "    text = whitespace.sub(' ', text)                           # substitui espa√ßos em branco por ' '\n",
    "    text = user.sub('', text)                                  # remove todas as men√ß√µes de usu√°rio encontradas no texto\n",
    "    text = re.sub(r\"\\[[^()]*\\]\",\"\", text)                      # remove o conte√∫do dentro de colchetes, incluindo os colchetes\n",
    "    text = re.sub(\"\\d+\", \"\", text)                             # remove todos os d√≠gitos num√©ricos do texto\n",
    "    text = re.sub(r'[^\\w\\s]','',text)                          # remove todos os caracteres que n√£o s√£o palavras (letras e n√∫meros) ou espa√ßos em branco.\n",
    "    text = re.sub(r\"(?:@\\S*|#\\S*|http(?=.*://)\\S*)\", \"\", text) # remove men√ß√µes de usu√°rio, hashtags e URLs.\n",
    "    text = text.lower()                                        # texto para minusculo\n",
    "\n",
    "    # removendo as stop words\n",
    "    text = [word for word in text.split() if word not in list(STOPWORDS)]\n",
    "\n",
    "    # word lemmatization\n",
    "    sentence = []\n",
    "    for word in text:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sentence.append(lemmatizer.lemmatize(word,'v'))\n",
    "\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac42bf",
   "metadata": {},
   "source": [
    "#### **Aula 4 - Redes recorrentes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf9a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o DF com a base BBC TEXT\n",
    "link = \"https://raw.githubusercontent.com/RicardViana/fiap-Deep-Learning-and-unstructured-data/refs/heads/main/bbc-text.csv\"\n",
    "df = pd.read_csv(link, sep=\",\")\n",
    "\n",
    "# Fazer uma copia do DF caso precise consultar os dados originais\n",
    "df_2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a27351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver o data frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892fd6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver a qtd de linhas e colunas\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver os dados unico da category\n",
    "df['category'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver os dados unico \n",
    "set(df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f8c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver a distribui√ß√£o da category em um grafico \n",
    "plt.figure(figsize=(8,3))\n",
    "sns.countplot(data=df, x='category', hue='category')\n",
    "plt.title(\"Total de not√≠cias por classe\", size=15)\n",
    "plt.xlabel(\"Catgorias das classes\", size=14)\n",
    "plt.xticks(rotation=25)\n",
    "plt.ylabel(\"N√∫mero de not√≠cias\", size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar o texto antes e depois da limpeza\n",
    "print(\"Texto antes da limpeza:\\n\",df['text'][0])\n",
    "print(\"---\"*100)\n",
    "print(\"Texto depois da limpeza:\\n\",limpeza_texto(df['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dbf0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a fun√ß√£o no dataframe inteiro\n",
    "df['text'] = df['text'].apply(limpeza_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f65d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprimento do total de caracteres antes e depois da limpeza dos dados de texto\n",
    "antigo_total_caracter = df_2['text'].apply(len).sum()\n",
    "novo_total_caracter = df['text'].apply(len).sum()\n",
    "\n",
    "print(f\"Tamanho de caracteres antes da limpeza: {antigo_total_caracter}\")\n",
    "print(f\"Tamanho de caracteres depois da limpeza: {novo_total_caracter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2dd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow de tokeniza√ß√£o e vetoriza√ß√£o\n",
    "# codifica√ß√£o one-hot de n√≠vel de palavra para dados de amostra\n",
    "\n",
    "samples = list(df['text'][:5].values)  # amostras dos primeiros cinco documentos do nosso conjunto de dados\n",
    "\n",
    "token_index = {} # cria um √≠ndice de tokens nos dados\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1 # atribuindo √≠ndice exclusivo para cada palavra √∫nica\n",
    "\n",
    "max_length = 15 # Define um valor m√°ximo para o comprimento das sequ√™ncias. Todas as sequ√™ncias ter√£o esse comprimento, e as palavras adicionais ser√£o truncadas se necess√°rio\n",
    "\n",
    "results = np.zeros(shape=(len(samples),   # resultados ser√£o armazenados neste array multidimensional de zeros com as dimens√µes obtidas pelo tamanho das amostras e o tamanho m√°ximo de comprimento\n",
    "                          max_length,     # Esta matriz ser√° usada para armazenar a codifica√ß√£o one-hot das palavras em cada sequ√™ncia de amostra\n",
    "                          max(token_index.values()) +1))\n",
    "\n",
    "print(\"Shape de resultados armazenados:\", results.shape)\n",
    "print(\"√çndice de token de palavras √∫nicas: \\n\", token_index)\n",
    "\n",
    "# criando a matriz one hot econdig\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i,j,index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567cd4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando LabelEncoder nas categorias para converter r√≥tulos de classes de texto ou categorias em n√∫meros inteiros\n",
    "X = df['text']\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df['category'])\n",
    "\n",
    "print(\"tamanho dos dados de entrada: \", X.shape)\n",
    "print(\"tamanho da vari√°vel alvo: \", y.shape)\n",
    "\n",
    "# Seprando os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "# Criando a tokeniza√ß√£o\n",
    "# Tendo agora as palavras em uma grande lista (1000), usamos a classe Tokenizer para tokeniza√ß√£o das palavras e cria√ß√£o de um vocabul√°rio\n",
    "# com as 1000 palavras mais frequentes no texto(par√¢metro num_words do Tokenizer):\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token='<00V>')  # OOV = out of vocabulary (fora de vocabul√°rio)\n",
    "tokenizer.fit_on_texts(X_train) # construindo o √≠ndice de palavras\n",
    "\n",
    "# Preenchimento de dados de entrada de texto X_train\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) #converte strings em listas inteiras\n",
    "train_padseq = pad_sequences(train_seq, maxlen=200) # preenche as listas de inteiros para o tensor de inteiros 2D\n",
    "# maxlen define o comprimento m√°ximo desejado para as sequ√™ncias ap√≥s o preenchimento (padding)\n",
    "\n",
    "# preenchimento de dados de entrada de texto X_test\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "test_padseq = pad_sequences(test_seq, maxlen=200)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 1500  # n√∫mero total de palavras a serem consideradas na camada de incorpora√ß√£o\n",
    "total_words = len(word_index)\n",
    "maxlen = 200 # comprimento m√°ximo da sequ√™ncia\n",
    "y_train = to_categorical(y_train, num_classes=5)\n",
    "y_test = to_categorical(y_test, num_classes=5)\n",
    "print(\"Tamanho do √≠ndice de palavras:\", total_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambiente_fase_5_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
